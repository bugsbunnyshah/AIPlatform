////
This guide is maintained in the main Braineous repository
and pull requests should be submitted there:
https://github.com/bugsbunnyshah/AIPlatform/tree/master/documentation/src/main/asciidoc
////
= Braineous, AI Platform - Data Ingestion

include::./attributes.adoc[]

:toc: macro
:toclevels: 4
:doctype: book
:icons: font
:docinfo1:

:numbered:
:sectnums:
:sectnumlevels: 4


Braineous, AI Platform - Data Ingestion
This guide covers:

* Running the Platform as a Microservice
* Designing and Developing a Data Ingestion Application

== Prerequisites

To complete this guide, you need:

* less than 15 minutes
* an IDE
* JDK 8 or 11+ installed with `JAVA_HOME` configured appropriately
* Apache Maven {maven-version}

[TIP]
.Verify Maven is using the Java you expect
====
If you have multiple JDK's installed it is not certain Maven will pick up the expected java
and you could end up with unexpected results.
You can verify which JDK Maven uses by running `mvn --version`. 
====

== Running the Platform as a Microservice
[source,bash,subs=attributes+]
----
java -jar aiplatform-secretariat-GA-runner.jar
----

== Designing and Developing a Data Ingestion Application

[source,java]
----
package io.appgallabs.tutorial;

import com.google.gson.JsonArray;
import com.google.gson.JsonObject;
import com.google.gson.JsonParser;

import org.apache.commons.io.IOUtils;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.util.ModelSerializer;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.learning.config.Nesterovs;
import org.nd4j.linalg.lossfunctions.LossFunctions;

import java.io.ByteArrayOutputStream;
import java.net.URI;
import java.net.http.HttpClient;
import java.net.http.HttpRequest;
import java.net.http.HttpResponse;
import java.nio.charset.StandardCharsets;
import java.util.Base64;
import java.util.UUID;

public class MLWorkflowWithIngestion {
    private static Logger logger = LoggerFactory.getLogger(MLWorkflowWithIngestion.class);

    private static String principal = "PAlDekAoo0XWjAicU9SQDKgy7B0y2p2t";
    private static String token = "";

    public static void main(String[] args) throws Exception
    {
        int batchSize = 50;
        int seed = 123;
        double learningRate = 0.005;
        int nEpochs = 30;

        int numInputs = 2;
        int numOutputs = 2;
        int numHiddenNodes = 20;

        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .weightInit(WeightInit.XAVIER)
                .updater(new Nesterovs(learningRate, 0.9))
                .list()
                .layer(new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes)
                        .activation(Activation.RELU)
                        .build())
                .layer(new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                        .activation(Activation.SOFTMAX)
                        .nIn(numHiddenNodes).nOut(numOutputs).build())
                .build();


        MultiLayerNetwork model = new MultiLayerNetwork(conf);
        model.init();
        model.setListeners(new ScoreIterationListener(10));

        //Deploy the Model, for training in the Cloud
        ByteArrayOutputStream modelBytes = new ByteArrayOutputStream();
        ModelSerializer.writeModel(model, modelBytes, false);
        String modelString = Base64.getEncoder().encodeToString(modelBytes.toByteArray());

        JsonObject jsonObject = new JsonObject();
        jsonObject.addProperty("name", UUID.randomUUID().toString());
        jsonObject.addProperty("model", modelString);
        logger.info(jsonObject.toString());

        JsonObject response = saveModelToCloud(jsonObject.toString());
        logger.info(response.toString());
        long modelId = response.get("modelId").getAsLong();

        //Ingest the data into the dataLake that can be used for ML components,
        //And available for building Applications
        String data = IOUtils.toString(Thread.currentThread().
                        getContextClassLoader().
                        getResourceAsStream("dataScience/saturn_data_train.csv"),
                StandardCharsets.UTF_8
                );
        JsonObject input = new JsonObject();
        input.addProperty("sourceData", data);
        input.addProperty("hasHeader", false);
        logger.info(input.toString());
        response = ingestDataIntoDataLake(input.toString());
        logger.info(response.toString());


        //Launch training in the Cloud
        JsonObject trainingInput = new JsonObject();
        JsonArray trainingDataLakeIds = new JsonArray();
        trainingDataLakeIds.add(response.get("dataLakeId").getAsString());
        trainingInput.addProperty("modelId", modelId);
        trainingInput.add("dataLakeIds", trainingDataLakeIds);
        logger.info(trainingInput.toString());
        response = trainModelInCloud(trainingInput.toString());
        logger.info(response.toString());

        //Deploy the trained model as a live model
        JsonObject deploy = new JsonObject();
        deploy.addProperty("modelId", modelId);
        logger.info("DEPLOY_REQUEST: "+deploy.toString());
        response = deployLiveModelInCloud(deploy.toString());
        logger.info("DEPLOY_RESPONSE: "+response.toString());

        //Ingest the data that will call the live model
        data = IOUtils.toString(Thread.currentThread().
                        getContextClassLoader().
                        getResourceAsStream("dataScience/saturn_data_eval.csv"),
                StandardCharsets.UTF_8
        );
        input = new JsonObject();
        input.addProperty("sourceData", data);
        input.addProperty("hasHeader", false);
        logger.info(input.toString());
        response = ingestDataIntoDataLake(input.toString());
        logger.info(response.toString());

        //Evaluate the Live Model in the Cloud
        JsonObject liveInput = new JsonObject();
        JsonArray liveDataSetIds = new JsonArray();
        liveDataSetIds.add(response.get("dataLakeId").getAsString());
        liveInput.addProperty("modelId", modelId);
        liveInput.add("dataLakeIds", liveDataSetIds);
        logger.info(liveInput.toString());
        response = evaluateLiveModelInCloud(liveInput.toString());
        logger.info(response.toString());

    }

    private static JsonObject saveModelToCloud(String payload) throws Exception
    {
        //Create the Experiment
        HttpClient httpClient = HttpClient.newBuilder().build();
        String restUrl = "http://localhost:8080/aimodel/performPackaging/";

        HttpRequest.Builder httpRequestBuilder = HttpRequest.newBuilder();
        HttpRequest httpRequest = httpRequestBuilder.uri(new URI(restUrl))
                .header("Content-Type", "application/json")
                .header("Principal", principal)
                .header("Bearer","")
                .POST(HttpRequest.BodyPublishers.ofString(payload))
                .build();

        HttpResponse<String> httpResponse = httpClient.send(httpRequest, HttpResponse.BodyHandlers.ofString());
        String responseJson = httpResponse.body();

        return JsonParser.parseString(responseJson).getAsJsonObject();
    }

    private static JsonObject ingestDataIntoDataLake(String payload) throws Exception
    {
        //Create the Experiment
        HttpClient httpClient = HttpClient.newBuilder().build();
        String restUrl = "http://localhost:8080/dataMapper/mapCsv/";

        HttpRequest.Builder httpRequestBuilder = HttpRequest.newBuilder();
        HttpRequest httpRequest = httpRequestBuilder.uri(new URI(restUrl))
                .header("Content-Type", "application/json")
                .header("Principal", principal)
                .header("Bearer","")
                .POST(HttpRequest.BodyPublishers.ofString(payload))
                .build();

        HttpResponse<String> httpResponse = httpClient.send(httpRequest, HttpResponse.BodyHandlers.ofString());
        String responseJson = httpResponse.body();

        return JsonParser.parseString(responseJson).getAsJsonObject();
    }

    private static JsonObject trainModelInCloud(String payload) throws Exception
    {
        //Create the Experiment
        HttpClient httpClient = HttpClient.newBuilder().build();
        String restUrl = "http://localhost:8080/trainModel/trainJavaFromDataLake/";

        HttpRequest.Builder httpRequestBuilder = HttpRequest.newBuilder();
        HttpRequest httpRequest = httpRequestBuilder.uri(new URI(restUrl))
                .header("Content-Type", "application/json")
                .header("Principal", principal)
                .header("Bearer","")
                .POST(HttpRequest.BodyPublishers.ofString(payload))
                .build();

        HttpResponse<String> httpResponse = httpClient.send(httpRequest, HttpResponse.BodyHandlers.ofString());
        String responseJson = httpResponse.body();

        return JsonParser.parseString(responseJson).getAsJsonObject();
    }

    private static JsonObject deployLiveModelInCloud(String payload) throws Exception
    {
        //Create the Experiment
        HttpClient httpClient = HttpClient.newBuilder().build();
        String restUrl = "http://localhost:8080/liveModel/deployJavaModel/";

        HttpRequest.Builder httpRequestBuilder = HttpRequest.newBuilder();
        HttpRequest httpRequest = httpRequestBuilder.uri(new URI(restUrl))
                .header("Content-Type", "application/json")
                .header("Principal", principal)
                .header("Bearer","")
                .POST(HttpRequest.BodyPublishers.ofString(payload))
                .build();

        HttpResponse<String> httpResponse = httpClient.send(httpRequest, HttpResponse.BodyHandlers.ofString());
        String responseJson = httpResponse.body();

        return JsonParser.parseString(responseJson).getAsJsonObject();
    }

    private static JsonObject evaluateLiveModelInCloud(String payload) throws Exception
    {
        //Create the Experiment
        HttpClient httpClient = HttpClient.newBuilder().build();
        String restUrl = "http://localhost:8080/liveModel/evalJavaFromDataLake/";

        HttpRequest.Builder httpRequestBuilder = HttpRequest.newBuilder();
        HttpRequest httpRequest = httpRequestBuilder.uri(new URI(restUrl))
                .header("Content-Type", "application/json")
                .header("Principal", principal)
                .header("Bearer","")
                .POST(HttpRequest.BodyPublishers.ofString(payload))
                .build();

        HttpResponse<String> httpResponse = httpClient.send(httpRequest, HttpResponse.BodyHandlers.ofString());
        String responseJson = httpResponse.body();

        return JsonParser.parseString(responseJson).getAsJsonObject();
    }
}
----

== Data Ingestion Explained

Any Artificial Intelligence Product needs real data to learn.
One of the bigger challenges is integrating with Customer Data.
ETL has been in use for years, but the cost of doing those integrations are quite high.
Hence, we took a different approach. We use smart schema mappers in the Cloud to provision your data in a way that fits your AI model.
link:http://openii.sourceforge.net/index.php?act=tools&page=harmony[OpenII Harmony Framework, window="_blank"] is very effective for this. Braineous supports data ingestion in three formats â€” Json, XML, and CSV.
Upon ingestion the data is added to your private and secure DataLake backed by MongoDB.
Later you can use this data to produce datasets to train your AI models.

== What's next?

This guide covered the creation of a Data Ingestion Application using Braineous.
link:datascience.html[Data Science Application].
